{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Preparation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\shreyash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\shreyash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading treebank and universal tagset\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')], [('The', 'DET'), ('asbestos', 'NOUN'), ('fiber', 'NOUN'), (',', '.'), ('crocidolite', 'NOUN'), (',', '.'), ('is', 'VERB'), ('unusually', 'ADV'), ('resilient', 'ADJ'), ('once', 'ADP'), ('it', 'PRON'), ('enters', 'VERB'), ('the', 'DET'), ('lungs', 'NOUN'), (',', '.'), ('with', 'ADP'), ('even', 'ADV'), ('brief', 'ADJ'), ('exposures', 'NOUN'), ('to', 'PRT'), ('it', 'PRON'), ('causing', 'VERB'), ('symptoms', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('show', 'VERB'), ('up', 'PRT'), ('decades', 'NOUN'), ('later', 'ADJ'), (',', '.'), ('researchers', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('*T*-2', 'X'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "print(nltk_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Splitting Data into Train and Validation set </b><br>Sample size - 95:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "Train_set, Validation_set = train_test_split(nltk_data,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of Train set :\n",
      "3718\n",
      "len of Validation set :\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print(\"len of Train set :\")\n",
    "print(len(Train_set))\n",
    "print(\"len of Validation set :\")\n",
    "print(len(Validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95939"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in Train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Copperweld', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('it', 'PRON'),\n",
       " ('does', 'VERB'),\n",
       " (\"n't\", 'ADV'),\n",
       " ('expect', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('protracted', 'ADJ'),\n",
       " ('strike', 'NOUN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "\n",
    "# vocabulary\n",
    "Vocab = set(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tags:\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "noOfTags = set([pair[1] for pair in train_tagged_words])\n",
    "print(\"Number of Tags:\")\n",
    "print(len(noOfTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Tags:\n",
      "['ADJ', 'PRT', 'VERB', 'ADP', 'CONJ', 'ADV', 'PRON', '.', 'NOUN', 'X', 'DET', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "print (\"List of Tags:\")\n",
    "print(list(noOfTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Tags:\n",
      "[('NOUN', 27485), ('VERB', 12916), ('.', 11180), ('ADP', 9389), ('DET', 8318)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "frequentTags = Counter([pair[1] for pair in train_tagged_words])\n",
    "print(\"Most Frequent Tags:\")\n",
    "print(frequentTags.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>POS Tagging - HMM </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Token X Vocab matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = len(noOfTags)\n",
    "v = len(Vocab)\n",
    "WordTagMatrix = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Emission Probabilities </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Transition Probabilities </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(noOfTags), len(noOfTags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(noOfTags)):\n",
    "    for j, t2 in enumerate(list(noOfTags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.DataFrame(tags_matrix, columns = list(noOfTags), index=list(noOfTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADP</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>.</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>DET</th>\n",
       "      <th>NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.064183</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>0.012147</td>\n",
       "      <td>0.077643</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.064839</td>\n",
       "      <td>0.701740</td>\n",
       "      <td>0.021011</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.021011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.400195</td>\n",
       "      <td>0.021456</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.249675</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.100455</td>\n",
       "      <td>0.057542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.065190</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.168396</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.081217</td>\n",
       "      <td>0.035615</td>\n",
       "      <td>0.034841</td>\n",
       "      <td>0.111257</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.134252</td>\n",
       "      <td>0.023150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.106082</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.016935</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.069549</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>0.321333</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.324103</td>\n",
       "      <td>0.062733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.120460</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>0.156782</td>\n",
       "      <td>0.054253</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.054713</td>\n",
       "      <td>0.058851</td>\n",
       "      <td>0.035402</td>\n",
       "      <td>0.345747</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>0.120920</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.128027</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.344942</td>\n",
       "      <td>0.120066</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.079602</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.032172</td>\n",
       "      <td>0.022554</td>\n",
       "      <td>0.068325</td>\n",
       "      <td>0.032836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.073552</td>\n",
       "      <td>0.012576</td>\n",
       "      <td>0.486662</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.034680</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.040396</td>\n",
       "      <td>0.209604</td>\n",
       "      <td>0.090701</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>0.006860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.044275</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.089445</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>0.058855</td>\n",
       "      <td>0.052504</td>\n",
       "      <td>0.066279</td>\n",
       "      <td>0.093918</td>\n",
       "      <td>0.219320</td>\n",
       "      <td>0.027191</td>\n",
       "      <td>0.173792</td>\n",
       "      <td>0.079964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.145934</td>\n",
       "      <td>0.176533</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.241004</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.009351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.017137</td>\n",
       "      <td>0.184227</td>\n",
       "      <td>0.204697</td>\n",
       "      <td>0.144716</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>0.162805</td>\n",
       "      <td>0.063313</td>\n",
       "      <td>0.074262</td>\n",
       "      <td>0.054745</td>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.205338</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.040154</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.637052</td>\n",
       "      <td>0.045804</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.034155</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>0.018414</td>\n",
       "      <td>0.035343</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.117315</td>\n",
       "      <td>0.351945</td>\n",
       "      <td>0.210870</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.183546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADJ       PRT      VERB       ADP      CONJ       ADV      PRON  \\\n",
       "ADJ   0.064183  0.010506  0.012147  0.077643  0.017236  0.004432  0.000657   \n",
       "PRT   0.084200  0.001951  0.400195  0.021456  0.001951  0.009753  0.018531   \n",
       "VERB  0.065190  0.031589  0.168396  0.090198  0.005420  0.081217  0.035615   \n",
       "ADP   0.106082  0.001491  0.008627  0.016935  0.000852  0.013420  0.069549   \n",
       "CONJ  0.120460  0.004138  0.156782  0.054253  0.000460  0.054713  0.058851   \n",
       "ADV   0.128027  0.014925  0.344942  0.120066  0.006633  0.079602  0.015257   \n",
       "PRON  0.073552  0.012576  0.486662  0.022485  0.004573  0.034680  0.008003   \n",
       ".     0.044275  0.002504  0.089445  0.091860  0.058855  0.052504  0.066279   \n",
       "NOUN  0.012261  0.044242  0.145934  0.176533  0.043005  0.017100  0.004657   \n",
       "X     0.017137  0.184227  0.204697  0.144716  0.009997  0.025547  0.055697   \n",
       "DET   0.205338  0.000240  0.040154  0.009257  0.000481  0.012623  0.003607   \n",
       "NUM   0.034155  0.026730  0.018414  0.035343  0.013662  0.002970  0.001485   \n",
       "\n",
       "             .      NOUN         X       DET       NUM  \n",
       "ADJ   0.064839  0.701740  0.021011  0.004596  0.021011  \n",
       "PRT   0.041612  0.249675  0.012679  0.100455  0.057542  \n",
       "VERB  0.034841  0.111257  0.218876  0.134252  0.023150  \n",
       "ADP   0.039940  0.321333  0.034934  0.324103  0.062733  \n",
       "CONJ  0.035402  0.345747  0.008276  0.120920  0.040000  \n",
       "ADV   0.134660  0.032172  0.022554  0.068325  0.032836  \n",
       "PRON  0.040396  0.209604  0.090701  0.009909  0.006860  \n",
       ".     0.093918  0.219320  0.027191  0.173792  0.079964  \n",
       "NOUN  0.241004  0.263889  0.028852  0.013171  0.009351  \n",
       "X     0.162805  0.063313  0.074262  0.054745  0.002856  \n",
       "DET   0.017793  0.637052  0.045804  0.005650  0.022000  \n",
       "NUM   0.117315  0.351945  0.210870  0.003564  0.183546  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Modelling <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vanila Viterbi POS tagger</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words):\n",
    "    state = []\n",
    "    Tags = list(set([pair[1] for pair in train_tagged_words]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        prob = [] \n",
    "        for tag in Tags:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            prob.append(state_probability)\n",
    "            \n",
    "        pmax = max(prob)\n",
    "        state_max = Tags[prob.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "# choose random 10 sents\n",
    "rndom = [random.randint(1,len(Validation_set)) for x in range(10)]\n",
    "validation_run = [Validation_set[i] for i in rndom]\n",
    "validation_run_base = [tup for sent in validation_run for tup in sent]\n",
    "validation_tagged_words = [tup[0] for sent in validation_run for tup in sent]\n",
    "len(validation_tagged_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'ADP'), ('this', 'DET'), ('one', 'NUM'), (',', '.'), ('the', 'DET'), ('screen', 'NOUN'), ('fills', 'VERB'), ('with', 'ADP'), ('photographs', 'NOUN'), ('of', 'ADP')]\n",
      "Accuracy:\n",
      "0.9170506912442397\n",
      "Runtime:\n",
      "28.750932693481445\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tagged_seq = Viterbi(validation_tagged_words)\n",
    "print(tagged_seq[:10])\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, validation_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "end = time.time()\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"Runtime:\")\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly tagged cases\n",
      "[[('this', 'DET'), (('one', 'NUM'), ('one', 'NOUN'))], [('*', 'X'), (('Possibly', 'ADJ'), ('Possibly', 'ADV'))], [('offsetting', 'VERB'), (('that', 'ADP'), ('that', 'DET'))], [('$', '.'), (('45.2', 'ADJ'), ('45.2', 'NUM'))], [('$', '.'), (('84.9', 'ADJ'), ('84.9', 'NUM'))], [('$', '.'), (('1.24', 'ADJ'), ('1.24', 'NUM'))], [('year', 'NOUN'), (('earlier', 'ADV'), ('earlier', 'ADJ'))], [('the', 'DET'), (('backdrop', 'ADJ'), ('backdrop', 'NOUN'))], [('.', '.'), (('Hiroshi', 'ADJ'), ('Hiroshi', 'NOUN'))], [('Hiroshi', 'NOUN'), (('Asada', 'ADJ'), ('Asada', 'NOUN'))], [('industrial', 'ADJ'), (('average', 'ADJ'), ('average', 'NOUN'))], [('takeover', 'NOUN'), (('targets', 'VERB'), ('targets', 'NOUN'))], [('.', '.'), (('Could', 'ADJ'), ('Could', 'VERB'))], [('evil', 'ADJ'), (('deeds', 'ADJ'), ('deeds', 'NOUN'))], [('of', 'ADP'), (('program-trading', 'ADJ'), ('program-trading', 'NOUN'))], [('program-trading', 'NOUN'), (('goblins', 'ADJ'), ('goblins', 'NOUN'))], [('the', 'DET'), (('genie', 'ADJ'), ('genie', 'NOUN'))], [('genie', 'NOUN'), (('back', 'NOUN'), ('back', 'ADV'))]]\n"
     ]
    }
   ],
   "source": [
    "#incorrectly tagged cases\n",
    "incorrect_tagged_cases = [[validation_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, validation_run_base)) if j[0]!=j[1]]\n",
    "print(\"Incorrectly tagged cases\")\n",
    "print(incorrect_tagged_cases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common incorrectly tagged cases are \n",
      "[(('$', '.'), 3), (('the', 'DET'), 2), (('.', '.'), 2), (('Hiroshi', 'NOUN'), 2), (('program-trading', 'NOUN'), 2)]\n"
     ]
    }
   ],
   "source": [
    "tag_list = []\n",
    "for case in incorrect_tagged_cases:\n",
    "    tag_list.append(case[0])\n",
    "    tag_list.append(case[1][0])\n",
    "    tag_list.append(case[1][1])\n",
    "tag_counts = Counter(tag_list)\n",
    "print(\"Most common incorrectly tagged cases are \")\n",
    "print(tag_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solving problems of unkown words </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram Tagger : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Unigram Tagger:\n",
      "0.8955034832172261\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(Train_set)\n",
    "accuracyUnigramTagger = unigram_tagger.evaluate(Validation_set)\n",
    "print(\"Accuracy of Unigram Tagger:\")\n",
    "print(accuracyUnigramTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying generic patterns\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerund\n",
    "    (r'.*ed$', 'VBD'),               # past tense\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*', 'NN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with lexicon and rule based tagger\n",
      "0.8955034832172261\n"
     ]
    }
   ],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "\n",
    "lexicon_tagger = nltk.UnigramTagger(Train_set, backoff=rule_based_tagger)\n",
    "\n",
    "accuracy_lexicon_tagger = lexicon_tagger.evaluate(Validation_set)\n",
    "print(\"Accuracy with lexicon and rule based tagger\")\n",
    "print(accuracy_lexicon_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bigram Tagger:\n",
      "0.8982478361832383\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(Train_set,backoff=lexicon_tagger)\n",
    "accuracy_bigram_tagger = bigram_tagger.evaluate(Validation_set)\n",
    "print(\"Accuracy of Bigram Tagger:\")\n",
    "print(accuracy_bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram Tagger:\n",
      "0.8982478361832383\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(Train_set,backoff=lexicon_tagger)\n",
    "accuracy_trigram_tagger = trigram_tagger.evaluate(Validation_set)\n",
    "print(\"Accuracy of trigram Tagger:\")\n",
    "print(accuracy_trigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trying to build Bigram tagger into viterbi</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying patterns to out data\n",
    "def bigram_tagger(word,train_set = Train_set):\n",
    "    patterns1 = [\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),               # past tense\n",
    "    (r'.*es$', 'VERB'),               # 3rd singular present\n",
    "    (r'.*ould$', 'X'),              # modals\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN')                    # nouns\n",
    "    ]\n",
    "    regex_based_tagger = nltk.RegexpTagger(patterns1)\n",
    "    \n",
    "    # trigram backed up by the regex tagger\n",
    "    bigram_regex_tagger = nltk.BigramTagger(train_set, backoff=regex_based_tagger)\n",
    "    return bigram_regex_tagger.tag_sents([[(word)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_modified(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        # unknown words from bigram taggr\n",
    "        if word not in tokens:\n",
    "            unk_word_tag=bigram_tagger(word)\n",
    "            for sent in unk_word_tag:\n",
    "                for tup in sent:\n",
    "                    state.append(tup[1])\n",
    "        # rest remains same            \n",
    "        else:            \n",
    "            p = [] \n",
    "            for tag in T:\n",
    "                if key == 0:\n",
    "                    transition_p = tags_df.loc['.', tag]\n",
    "                else:\n",
    "                    transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "                emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "                state_probability = emission_p * transition_p    \n",
    "                p.append(state_probability)\n",
    "            \n",
    "            pmax = max(p)\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "            \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq = Viterbi_modified(validation_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'one',\n",
       " ',',\n",
       " 'the',\n",
       " 'screen',\n",
       " 'fills',\n",
       " 'with',\n",
       " 'photographs',\n",
       " 'of',\n",
       " 'both',\n",
       " 'candidates',\n",
       " '.',\n",
       " '*',\n",
       " 'Possibly',\n",
       " 'offsetting',\n",
       " 'that',\n",
       " ',',\n",
       " 'Columbia',\n",
       " 'recently',\n",
       " 'estimated',\n",
       " '0',\n",
       " 'it',\n",
       " 'has',\n",
       " 'unrealized',\n",
       " 'gains',\n",
       " 'on',\n",
       " 'publicly',\n",
       " 'traded',\n",
       " 'equity',\n",
       " 'investments',\n",
       " 'of',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '70',\n",
       " 'million',\n",
       " '*U*',\n",
       " '.',\n",
       " 'IBM',\n",
       " ',',\n",
       " 'the',\n",
       " 'giant',\n",
       " 'computer',\n",
       " 'maker',\n",
       " ',',\n",
       " 'offered',\n",
       " '$',\n",
       " '750',\n",
       " 'million',\n",
       " '*U*',\n",
       " 'of',\n",
       " 'non-callable',\n",
       " '30-year',\n",
       " 'debentures',\n",
       " 'priced',\n",
       " '*',\n",
       " '*-1',\n",
       " 'to',\n",
       " 'yield',\n",
       " '8.47',\n",
       " '%',\n",
       " ',',\n",
       " 'or',\n",
       " 'about',\n",
       " '1\\\\/2',\n",
       " 'percentage',\n",
       " 'point',\n",
       " 'higher',\n",
       " 'than',\n",
       " 'the',\n",
       " 'yield',\n",
       " 'on',\n",
       " '30-year',\n",
       " 'Treasury',\n",
       " 'bonds',\n",
       " '.',\n",
       " 'The',\n",
       " 'St.',\n",
       " 'Louis',\n",
       " 'company',\n",
       " 'earned',\n",
       " '$',\n",
       " '45.2',\n",
       " 'million',\n",
       " '*U*',\n",
       " ',',\n",
       " 'or',\n",
       " '65',\n",
       " 'cents',\n",
       " 'a',\n",
       " 'share',\n",
       " ',',\n",
       " 'compared',\n",
       " 'with',\n",
       " '$',\n",
       " '84.9',\n",
       " 'million',\n",
       " '*U*',\n",
       " ',',\n",
       " 'or',\n",
       " '$',\n",
       " '1.24',\n",
       " '*U*',\n",
       " 'a',\n",
       " 'share',\n",
       " ',',\n",
       " 'a',\n",
       " 'year',\n",
       " 'earlier',\n",
       " '.',\n",
       " 'Nissan',\n",
       " 'cited',\n",
       " 'strong',\n",
       " 'domestic',\n",
       " 'sales',\n",
       " 'against',\n",
       " 'the',\n",
       " 'backdrop',\n",
       " 'of',\n",
       " 'continuous',\n",
       " 'economic',\n",
       " 'expansion',\n",
       " '.',\n",
       " 'Hiroshi',\n",
       " 'Asada',\n",
       " 'The',\n",
       " 'industrial',\n",
       " 'average',\n",
       " 'jumped',\n",
       " 'more',\n",
       " 'than',\n",
       " '41',\n",
       " 'points',\n",
       " 'Tuesday',\n",
       " 'as',\n",
       " 'speculators',\n",
       " 'rushed',\n",
       " '*-1',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'shares',\n",
       " 'of',\n",
       " 'potential',\n",
       " 'takeover',\n",
       " 'targets',\n",
       " '.',\n",
       " 'Could',\n",
       " 'rising',\n",
       " 'volatility',\n",
       " 'possibly',\n",
       " 'be',\n",
       " 'related',\n",
       " 'to',\n",
       " 'uncertainty',\n",
       " 'about',\n",
       " 'the',\n",
       " 'economics',\n",
       " 'of',\n",
       " 'stocks',\n",
       " ',',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'the',\n",
       " 'evil',\n",
       " 'deeds',\n",
       " 'of',\n",
       " 'program-trading',\n",
       " 'goblins',\n",
       " '?',\n",
       " 'First',\n",
       " 'of',\n",
       " 'America',\n",
       " 'Bank',\n",
       " 'Corp.',\n",
       " 'said',\n",
       " '0',\n",
       " 'it',\n",
       " 'completed',\n",
       " 'its',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'Midwest',\n",
       " 'Financial',\n",
       " 'Group',\n",
       " 'Inc.',\n",
       " 'for',\n",
       " 'about',\n",
       " '$',\n",
       " '250',\n",
       " 'million',\n",
       " '*U*',\n",
       " '.',\n",
       " 'Many',\n",
       " 'people',\n",
       " ',',\n",
       " 'including',\n",
       " 'the',\n",
       " 'Big',\n",
       " 'Board',\n",
       " ',',\n",
       " 'think',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'too',\n",
       " 'late',\n",
       " '*',\n",
       " 'to',\n",
       " 'put',\n",
       " 'the',\n",
       " 'genie',\n",
       " 'back',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bottle',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_run_base = [tup for sent in validation_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "validation_tagged_words = [tup[0] for sent in validation_run for tup in sent]\n",
    "validation_tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq_modified = Viterbi_modified(validation_tagged_words)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of modified Viterbi:\n",
      "0.9493087557603687\n",
      "Time taken:\n",
      "38.270033836364746\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "check1 = [i for i, j in zip(tagged_seq_modified, validation_run_base) if i == j]\n",
    "accuracy_viterbi_modified = len(check1)/len(tagged_seq_modified)\n",
    "print(\"Accuracy of modified Viterbi:\")\n",
    "print(accuracy_viterbi_modified)\n",
    "print(\"Time taken:\")\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Comparing Tagging Accuracies: </b>\n",
    "<br> Viterbi - 0.9170506912442397\n",
    "<br> Unigram - 0.8955034832172261\n",
    "<br> Bigram - 0.8982478361832383\n",
    "<br> Trigram - 0.8982478361832383\n",
    "<br> Viterbi Modified - 0.9493087557603687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Sampling some cases where modified viterbi might have rectified the tagging </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 'ADJ'), (',', '.'), ('Twitter', 'ADJ'), (',', '.'), ('Facebook', 'ADJ'), (',', '.'), ('Instagram', 'ADJ'), (',', '.'), ('Whatsapp', 'ADJ'), ('are', 'VERB'), ('the', 'DET'), ('various', 'ADJ'), ('social', 'ADJ'), ('media', 'NOUN'), ('platforms', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "sentence_1 = \"Google, Twitter, Facebook, Instagram, Whatsapp are the various social media platforms.\"\n",
    "words = word_tokenize(sentence_1)\n",
    "tagged_seq = Viterbi(words)\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 'NOUN'), (',', '.'), ('Twitter', 'NOUN'), (',', '.'), ('Facebook', 'NOUN'), (',', '.'), ('Instagram', 'NOUN'), (',', '.'), ('Whatsapp', 'NOUN'), ('are', 'VERB'), ('the', 'DET'), ('various', 'ADJ'), ('social', 'ADJ'), ('media', 'NOUN'), ('platforms', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_seq_modified = Viterbi_modified(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'ADV'), ('with', 'ADP'), ('this', 'DET'), ('technology', 'NOUN'), ('called', 'VERB'), ('money', 'NOUN'), (',', '.'), ('trade', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('difficult', 'ADJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_1 = \"Even with this technology called money, trade has been difficult.\"\n",
    "words = word_tokenize(sentence_1)\n",
    "tagged_seq = Viterbi(words)\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'ADV'), ('with', 'ADP'), ('this', 'DET'), ('technology', 'NOUN'), ('called', 'VERB'), ('money', 'NOUN'), (',', '.'), ('trade', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('difficult', 'ADJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_seq_modified = Viterbi_modified(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('upGrad', 'ADJ'), ('is', 'VERB'), ('an', 'DET'), ('online', 'ADJ'), ('higher', 'ADJ'), ('education', 'NOUN'), ('platform', 'ADJ'), ('providing', 'VERB'), ('rigorous', 'ADJ'), ('industry-relevant', 'ADJ'), ('programs', 'NOUN'), ('designed', 'VERB'), ('and', 'CONJ'), ('delivered', 'VERB'), ('in', 'ADP'), ('collaboration', 'ADJ'), ('with', 'ADP'), ('world-class', 'ADJ'), ('faculty', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"upGrad is an online higher education platform providing rigorous industry-relevant programs designed and delivered in collaboration with world-class faculty\"\n",
    "words = word_tokenize(sentence_1)\n",
    "tagged_seq = Viterbi(words)\n",
    "print(tagged_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('upGrad', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('higher', 'ADJ'), ('education', 'NOUN'), ('platform', 'NOUN'), ('providing', 'VERB'), ('rigorous', 'NOUN'), ('industry-relevant', 'NOUN'), ('programs', 'NOUN'), ('designed', 'VERB'), ('and', 'CONJ'), ('delivered', 'VERB'), ('in', 'ADP'), ('collaboration', 'NOUN'), ('with', 'ADP'), ('world-class', 'NOUN'), ('faculty', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_seq_modified = Viterbi_modified(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
